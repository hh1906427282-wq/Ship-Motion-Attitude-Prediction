import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from model.predictor import Predictor
from data.dataset import load_data
import matplotlib.pyplot as plt
import joblib
import numpy as np

# ---------- è®­ç»ƒå‚æ•° ----------
input_len = 300
pred_len = 100
input_dim = 18
batch_size = 32
epochs = 300
lr = 0.001
patience = 15
min_delta = 1e-4

# ---------- å‡†å¤‡è®¾å¤‡ ----------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---------- åŠ è½½æ•°æ® ----------
train_dataset, test_dataset, scaler = load_data(
    csv_path='../å¤šå°ºåº¦ CNN + MI-BiLSTM + transformer + MLP è¾“å‡ºï¼ˆå»é™¤attentionæ³¨æ„åŠ›èåˆï¼‰/äº”çº§æµ·å†µ.csv',
    input_len=input_len,
    pred_len=pred_len,
    normalize=True
)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# ---------- åˆå§‹åŒ–æ¨¡å‹ ----------
model = Predictor(input_dim=input_dim, pred_len=pred_len).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# ---------- EarlyStopping ----------
best_loss = float('inf')
trigger_count = 0
train_loss_list = []

# ---------- è®­ç»ƒå¾ªç¯ ----------
os.makedirs("checkpoints", exist_ok=True)

for epoch in range(epochs):
    model.train()
    total_loss = 0

    for x, y in train_loader:
        x, y = x.to(device), y.to(device)

        optimizer.zero_grad()
        y_pred, _ = model(x)  # âœ… åªå–é¢„æµ‹ç»“æœï¼Œä¸ç”¨æ³¨æ„åŠ›
        loss_x = criterion(y_pred[:, :, 0:6],  y[:, :, 0:6])    # ä½ç½®
        loss_v = criterion(y_pred[:, :, 6:12], y[:, :, 6:12])   # é€Ÿåº¦
        loss_a = criterion(y_pred[:, :, 12:18], y[:, :, 12:18]) # åŠ é€Ÿåº¦
        loss = loss_x + loss_v + loss_a

        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    train_loss_list.append(avg_loss)
    print(f"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.6f} | x: {loss_x.item():.4f} | v: {loss_v.item():.4f} | a: {loss_a.item():.4f}")

    # ---------- EarlyStopping åˆ¤æ–­ ----------
    if avg_loss + min_delta < best_loss:
        best_loss = avg_loss
        trigger_count = 0
        torch.save(model.state_dict(), 'checkpoints/best_model.pth')
        print("âœ… Best model saved.")
    else:
        trigger_count += 1
        print(f"â†ªï¸ æ— æå‡ï¼ŒEarlyStopping è®¡æ•°ï¼š{trigger_count}/{patience}")
        if trigger_count >= patience:
            print("ğŸ›‘ è§¦å‘ EarlyStoppingï¼Œè®­ç»ƒæå‰ç»“æŸ")
            break

# ---------- ä¿å­˜æŸå¤±å›¾ ----------
plt.figure(figsize=(8, 4))
plt.plot(train_loss_list, label='Train Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Curve')
plt.grid(True)
plt.legend()
os.makedirs("loss_figs", exist_ok=True)
plt.savefig("loss_figs/train_loss_curve.png", dpi=300)
print("ğŸ“‰ è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜ï¼šloss_figs/train_loss_curve.png")

# ---------- ä¿å­˜ scaler å’Œæµ‹è¯•æ•°æ® ----------
joblib.dump(scaler, 'scaler_cnn_bilstm_attention.save')
np.save("test_input_data.npy", test_dataset.data)
print("âœ… scaler ä¿å­˜ä¸º scaler_cnn_bilstm_attention.save")
print("âœ… æµ‹è¯•æ•°æ®ä¿å­˜ä¸º test_input_data.npy")
